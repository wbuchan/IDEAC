{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports libraries and loads data into pandas dataframes\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas.io.json import json_normalize\n",
    "from pandas.tseries import converter as pdtc\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.units as munits\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout, Lambda, BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import keras\n",
    "import json\n",
    "import os\n",
    "import sklearn\n",
    "import numpy\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import percent_rank\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#Initialize spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ensemble_learning_sparktest\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def concatenate_json_to_df(directory_path, to_output, folder_output):\n",
    "    list_json = []\n",
    "    \n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.split(\".\")[-1] == \"json\":\n",
    "            \n",
    "            with open(directory_path+file, 'r') as f:\n",
    "            \n",
    "                # If json file is not empty\n",
    "                if os.stat(directory_path+file).st_size != 0:\n",
    "                                \n",
    "                        \n",
    "                    # For Observation file type\n",
    "                    if directory_path.split(\"/\")[-2] == \"Observations\":\n",
    "                        dictio = json.load(f)\n",
    "                                            \n",
    "                        if 'observation' in dictio:\n",
    "                            observation = dictio[\"observation\"]\n",
    "                        \n",
    "                            for entry in observation:\n",
    "                                entry['date'] =  file.rstrip(\".json\")\n",
    "                                entry['station'] = dictio['station']\n",
    "                                entry['time'] = datetime.utcfromtimestamp(entry['time']).strftime('%d-%m-%Y %H:%M:%S')\n",
    "                                list_json.append(entry)\n",
    "                    \n",
    "                    \n",
    "                    # For Analyses file type\n",
    "                    elif directory_path.split(\"/\")[-2] == \"Analyses\":\n",
    "                        dictio = json.load(f)\n",
    "                        if 'source' in dictio and 'temperature' in dictio:\n",
    "                            source = list(dictio['source'])\n",
    "                            temperature = list(dictio['temperature'])\n",
    "                            \n",
    "                            dictio['date'] = file.rstrip(\".json\")\n",
    "                            dictio.pop('source', None)\n",
    "                            dictio.pop('temperature', None)\n",
    "                            \n",
    "                            entry = dict()\n",
    "                            for i in range(0, len(source)):\n",
    "                                dictio[source[i]] = temperature[i]\n",
    "                                list_json.append(dictio)\n",
    "\n",
    "                                \n",
    "                    # For Previsions file type\n",
    "                    elif \"Previsions\" in directory_path.split(\"/\"):\n",
    "                        dictio = json.load(f)\n",
    "                        \n",
    "                        latit_r = None\n",
    "                        long_r = None\n",
    "                        \n",
    "                        if \"longitude_r\" in dictio:\n",
    "                            long_r = dictio['longitude_r']\n",
    "\n",
    "                        if \"latitude_r\" in dictio:\n",
    "                            latit_r = dictio['latitude_r']\n",
    "                            \n",
    "                        if 'forecast' in dictio:\n",
    "                            forecast = dictio['forecast'] \n",
    "                            \n",
    "                            \n",
    "                            for i in range(0, len(forecast)):\n",
    "                                dictio = dict()\n",
    "                                \n",
    "                                if long_r != None and latit_r != None:\n",
    "                                    dictio['latitude_r'] = latit_r\n",
    "                                    dictio['longitude_r'] = long_r\n",
    "                                \n",
    "                                if \"temperature\" in forecast[i]:\n",
    "                                    dictio[\"temperature\"] = forecast[i][\"temperature\"]\n",
    "                                    \n",
    "                                if \"time\" in forecast[i]:\n",
    "                                    dictio[\"time\"] = datetime.utcfromtimestamp(forecast[i][\"time\"]).strftime('%d-%m-%Y %H:%M:%S')\n",
    "                                    \n",
    "                                if \"wind\" in forecast[i]:\n",
    "                                    dictio[\"wind\"] = forecast[i][\"wind\"]\n",
    "                                \n",
    "                                if \"wind_dir\" in forecast[i]:\n",
    "                                    dictio[\"wind_dir\"] = forecast[i][\"wind_dir\"]\n",
    "                                \n",
    "                                if \"humidity\" in forecast[i]:\n",
    "                                    dictio[\"humidity\"] = forecast[i][\"humidity\"]\n",
    "                                \n",
    "                                if \"pressure\" in forecast[i]:\n",
    "                                            dictio[\"pressure\"] = forecast[i][\"pressure\"]\n",
    "                                        \n",
    "                                if \"cloud_cover\" in forecast[i]:\n",
    "                                            dictio[\"cloud_cover\"] = forecast[i][\"cloud_cover\"]\n",
    "\n",
    "    \n",
    "                                \n",
    "                                dictio[\"date\"] = file.rstrip(\".json\")\n",
    "                                \n",
    "                                list_json.append(dictio)\n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    dictio = dict()\n",
    "                    dictio[\"date\"] = file.rstrip(\".json\")\n",
    "                    list_json.append(dictio)\n",
    "                                    \n",
    "    df = pd.DataFrame(list_json)\n",
    "    df.drop_duplicates()\n",
    "    df.to_csv(folder_output+\"/\"+to_output.replace(\"/\", \"-\")+\".csv\", sep=\";\", index=False)\n",
    "    \n",
    "    \n",
    "dirname = \"technical_challenge_v9\"\n",
    "if not os.path.exists(dirname):\n",
    "    os.mkdir(dirname)\n",
    "    \n",
    "concatenate_json_to_df('technical_challenge/Asport CAPRED/Previsions/Api-Agro/', \"Asport CAPRED/Previsions/Api-Agro\", dirname)\n",
    "# Launch data of Api, Dark and weatherbit forcesaters for the station Asport\n",
    "path = \"technical_challenge_v9/Asport CAPRED-Previsions-\"\n",
    "path_previsions = [\"Api-Agro.csv\", \"Dark Sky.csv\", \"Weatherbit.csv\"]\n",
    "path_observations = \"technical_challenge_v9/Asport CAPRED-Observations.csv\"\n",
    "\n",
    "def get_forecast(path, path_previsions, path_observations):\n",
    "    list_data = []\n",
    "    for forecaster in path_previsions:\n",
    "        dataframe = pd.read_csv(path + forecaster, sep=\";\")\n",
    "        dataframe[\"time\"] = pd.to_datetime(dataframe['time'], format='%d-%m-%Y %H:%M:%S')\n",
    "        dataframe.sort_values(by=['time'])\n",
    "        dataframe['time'] = dataframe['time'].apply(lambda x: x.replace(minute=0, second=0))\n",
    "        dataframe = dataframe.groupby(['time']).apply(lambda x : x.iloc[:6].mean())\n",
    "        dataframe.columns = [x + \"_\" + forecaster.split('.')[0] for x in dataframe.columns]\n",
    "        list_data.append(dataframe)   \n",
    "    \n",
    "    observation_data = pd.read_csv(path_observations, sep=\";\", index_col=False).drop(columns=[\"station\"]).dropna()\n",
    "    observation_data[\"time\"] = pd.to_datetime(observation_data['time'], format='%d-%m-%Y %H:%M:%S')\n",
    "    observation_data.sort_values(by='time')\n",
    "    observation_data['time'] = observation_data['time'].apply(lambda x: x.replace(minute=0, second=0))\n",
    "    observation_data = observation_data.groupby(['time']).apply(lambda x : x.iloc[0])\n",
    "    observation_data=observation_data.rename(index=str, columns={\"humidity\": \"humidity_Y\", \"precipitation\": \"precipitation_Y\", \"temperature\":\"temperature_Y\"})\n",
    "    \n",
    "    return list_data, observation_data\n",
    "\n",
    "# Associate X to previsions and Y to observations \n",
    "list_data, observation_data = get_forecast(path, path_previsions, path_observations)\n",
    "X = pd.concat(list_data, axis=1)\n",
    "Y = observation_data\n",
    "\n",
    "# Delete non useful variable in prevision data\n",
    "X = X.loc[:, ~X.columns.str.startswith('l')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('c')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('p')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('wind_dir')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('prec')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('date')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('time')]\n",
    "\n",
    "# Delete non useful variable in observation data\n",
    "Y = Y.drop(['date','time'], axis=1)\n",
    "\n",
    "# Import JSON API in order to collect most recent data\n",
    "import json,urllib.request\n",
    "data_A = urllib.request.urlopen(\"http://sd-59247.dedibox.fr/agriscope/obs/transform_obs.php?STATION=Asport\").read()\n",
    "output_A = json.loads(data_A)\n",
    "\n",
    "# Transform JSON data in a dataframe A\n",
    "A = json.loads(data_A)\n",
    "A_data = pd.DataFrame(A)\n",
    "A = A_data.T\n",
    "\n",
    "# Delelte the index of the dataframe A\n",
    "A.index.name = 'time'\n",
    "A.reset_index(level=0, inplace=True)\n",
    "\n",
    "# Convert date time in correct format \n",
    "A['time'] = pd.to_datetime(A['time'])\n",
    "A['time'] = pd.to_datetime(A['time'],'%d-%m-%Y %H:%M:%S')\n",
    "A['time'] = A['time'].apply(lambda x: x.replace(minute=0, second=0))\n",
    "\n",
    "# Rename dataframe A columns\n",
    "A.columns = ['time', 'humidity_Y', 'precipitation_Y', 'temperature_Y']\n",
    "\n",
    "# Reset the index of the dataframe A\n",
    "A = A.set_index('time') \n",
    "\n",
    "\n",
    "# Concat vertically Y and A (Y contains former observation and A most recent one)\n",
    "frames = [Y, A]\n",
    "first_concat = pd.concat(frames)\n",
    "\n",
    "# Merge X and Y dataframe in one single dataframe \"df_forcast_cap\"\n",
    "df_forcast_cap = pd.merge(X, first_concat, right_index=True, left_index=True)\n",
    "df_forcast_cap = df_forcast_cap.drop(['precipitation_Y'], axis=1)\n",
    "\n",
    "# Delete missing values for \"df_forcast_cap\"\n",
    "df_forcast_cap = df_forcast_cap.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyarrow optimization for pandas/spark conversion \n",
    "\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") NEED TO FIX PyARROW VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep timestamps (spark dataframe deletes index)\n",
    "df_forcast_cap.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas to dataframe\n",
    "train_df = spark.createDataFrame(df_forcast_cap) #dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compress data points into feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"temperature_Api-Agro\", \"wind_Api-Agro\", \"humidity_Api-Agro\", \\\n",
    "                                      \"humidity_Dark Sky\", \"temperature_Dark Sky\", \"wind_Dark Sky\", \\\n",
    "                                      \"humidity_Weatherbit\", \"temperature_Weatherbit\", \"wind_Weatherbit\" \\\n",
    "                                      ],outputCol=\"features\")\n",
    "newdf = assembler.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates df with temperature_Y as label \n",
    "newdf = newdf.selectExpr(\"time\", \"temperature_Y as label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "newdf = newdf.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"time\")))\n",
    "\n",
    "trainingData = newdf.where(\"rank <= .7\").drop(\"rank\")\n",
    "testData = newdf.where(\"rank > .7\").drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"features\")\n",
    "model = rf.fit(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+--------------------+\n",
      "|               time|        prediction|label|            features|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "|2019-07-28 04:00:00| 76.62317284300812| 81.0|[19.7834315,9.896...|\n",
      "|2019-07-28 05:00:00| 76.62317284300812| 76.0|[19.8288008333333...|\n",
      "|2019-07-28 06:00:00| 73.79351481959176| 75.0|[20.1969648333333...|\n",
      "|2019-07-28 07:00:00| 67.34627463755837| 78.0|[21.2634120000000...|\n",
      "|2019-07-28 08:00:00| 60.65881085478945| 75.0|[22.6841473333333...|\n",
      "|2019-07-28 09:00:00| 54.91606831778425| 65.0|[24.0124353333333...|\n",
      "|2019-07-28 10:00:00|50.372995936193945| 56.0|[25.4114423333333...|\n",
      "|2019-07-28 11:00:00|46.233539642907274| 49.0|[26.5588219999999...|\n",
      "|2019-07-28 12:00:00| 43.31043530775608| 46.0|[27.6142416666666...|\n",
      "|2019-07-28 13:00:00| 39.68426682886637| 43.0|[28.3597656666666...|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 5.73981\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "rf_predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "rf_predictions.select(\"time\", \"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Compute RMS error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GBR\n",
    "gbr = GBTRegressor(featuresCol=\"features\", labelCol = \"label\", maxIter=50)\n",
    "\n",
    "# Train model using same data structure as RandomForest\n",
    "model = gbr.fit(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+--------------------+\n",
      "|               time|        prediction|label|            features|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "|2019-07-28 04:00:00| 77.35031147143562| 81.0|[19.7834315,9.896...|\n",
      "|2019-07-28 05:00:00| 77.31141289418515| 76.0|[19.8288008333333...|\n",
      "|2019-07-28 06:00:00| 73.86210317246814| 75.0|[20.1969648333333...|\n",
      "|2019-07-28 07:00:00| 68.44148026640768| 78.0|[21.2634120000000...|\n",
      "|2019-07-28 08:00:00|  61.5352699921056| 75.0|[22.6841473333333...|\n",
      "|2019-07-28 09:00:00|  60.6057524956332| 65.0|[24.0124353333333...|\n",
      "|2019-07-28 10:00:00| 53.84249972178284| 56.0|[25.4114423333333...|\n",
      "|2019-07-28 11:00:00| 50.56619925594794| 49.0|[26.5588219999999...|\n",
      "|2019-07-28 12:00:00|  46.8183786561112| 46.0|[27.6142416666666...|\n",
      "|2019-07-28 13:00:00|40.686906150113124| 43.0|[28.3597656666666...|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 3.99015\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "gbr_predictions = model.transform(testData)\n",
    "\n",
    "# Display example rows\n",
    "gbr_predictions.select(\"time\", \"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(gbr_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[time: timestamp, label: double, rf_prediction: double, gbr_prediction: double]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate results from RF, GBR\n",
    "\n",
    "# Isolate RF and GBR prediction columns\n",
    "rf_predictions = rf_predictions.drop(\"features\").withColumnRenamed(\"prediction\", \"rf_prediction\")\n",
    "gbr_predictions = gbr_predictions.drop(\"time\",\"label\",\"features\").withColumnRenamed(\"prediction\", \"gbr_prediction\")\n",
    "\n",
    "concatenated_predictions = rf_predictions.join(gbr_predictions)\n",
    "print(concatenated_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress features into feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"rf_prediction\", \"gbr_prediction\"],outputCol=\"features\")\n",
    "ndf = assembler.transform(concatenated_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create df with time, label (temperature_Y) and feature vector\n",
    "ndf = newdf.selectExpr(\"time\", \"label\", \"features\")\n",
    "ndf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "newdf = newdf.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"time\")))\n",
    "\n",
    "trainingData = newdf.where(\"rank <= .7\").drop(\"rank\")\n",
    "testData = newdf.where(\"rank > .7\").drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\")\n",
    "model = rf.fit(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+--------------------+\n",
      "|               time|        prediction|label|            features|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "|2019-07-28 04:00:00| 76.62317284300812| 81.0|[19.7834315,9.896...|\n",
      "|2019-07-28 05:00:00| 76.62317284300812| 76.0|[19.8288008333333...|\n",
      "|2019-07-28 06:00:00| 73.79351481959176| 75.0|[20.1969648333333...|\n",
      "|2019-07-28 07:00:00| 67.34627463755837| 78.0|[21.2634120000000...|\n",
      "|2019-07-28 08:00:00| 60.65881085478945| 75.0|[22.6841473333333...|\n",
      "|2019-07-28 09:00:00| 54.91606831778425| 65.0|[24.0124353333333...|\n",
      "|2019-07-28 10:00:00|50.372995936193945| 56.0|[25.4114423333333...|\n",
      "|2019-07-28 11:00:00|46.233539642907274| 49.0|[26.5588219999999...|\n",
      "|2019-07-28 12:00:00| 43.31043530775608| 46.0|[27.6142416666666...|\n",
      "|2019-07-28 13:00:00| 39.68426682886637| 43.0|[28.3597656666666...|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 5.73981\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "rf_predictions = model.transform(testData)\n",
    "\n",
    "# Display example rows\n",
    "rf_predictions.select(\"time\",\"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Compute RMS error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GBR\n",
    "gbr = GBTRegressor(featuresCol=\"features\", labelCol = \"label\", maxIter=50)\n",
    "\n",
    "# Train model using same data structure as RandomForest\n",
    "model = gbr.fit(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "| 77.35031147143562| 81.0|[19.7834315,9.896...|\n",
      "| 77.31141289418515| 76.0|[19.8288008333333...|\n",
      "| 73.86210317246814| 75.0|[20.1969648333333...|\n",
      "| 68.44148026640768| 78.0|[21.2634120000000...|\n",
      "|  61.5352699921056| 75.0|[22.6841473333333...|\n",
      "|  60.6057524956332| 65.0|[24.0124353333333...|\n",
      "| 53.84249972178284| 56.0|[25.4114423333333...|\n",
      "| 50.56619925594794| 49.0|[26.5588219999999...|\n",
      "|  46.8183786561112| 46.0|[27.6142416666666...|\n",
      "|40.686906150113124| 43.0|[28.3597656666666...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 3.99015\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "gbr_predictions = model.transform(testData)\n",
    "\n",
    "# Display example rows\n",
    "gbr_predictions.select(\"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(gbr_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[time: timestamp, label: double, rf_prediction_2: double]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate results from RF, GBR\n",
    "\n",
    "# Isolate RF and GBR prediction columns\n",
    "rf_predictions = rf_predictions.drop(\"features\").withColumnRenamed(\"prediction\", \"rf_prediction_2\")\n",
    "print(rf_predictions)\n",
    "gbr_predictions = gbr_predictions.drop(\"time\", \"label\",\"features\").withColumnRenamed(\"prediction\", \"gbr_prediction_2\")\n",
    "\n",
    "# Concatenate RF and GBR with previous features\n",
    "concatenated = rf_predictions.join(gbr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress data points into feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"rf_prediction_2\", \"gbr_prediction_2\"],outputCol=\"features\")\n",
    "ndf = assembler.transform(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create df with label and feature vector\n",
    "ndf = newdf.selectExpr(\"label\", \"features\")\n",
    "ndf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "newdf = newdf.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"time\")))\n",
    "\n",
    "trainingData = newdf.where(\"rank <= .7\").drop(\"rank\")\n",
    "testData = newdf.where(\"rank > .7\").drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\")\n",
    "model = rf.fit(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+--------------------+\n",
      "|               time|        prediction|label|            features|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "|2019-07-28 04:00:00| 76.62317284300812| 81.0|[19.7834315,9.896...|\n",
      "|2019-07-28 05:00:00| 76.62317284300812| 76.0|[19.8288008333333...|\n",
      "|2019-07-28 06:00:00| 73.79351481959176| 75.0|[20.1969648333333...|\n",
      "|2019-07-28 07:00:00| 67.34627463755837| 78.0|[21.2634120000000...|\n",
      "|2019-07-28 08:00:00| 60.65881085478945| 75.0|[22.6841473333333...|\n",
      "|2019-07-28 09:00:00| 54.91606831778425| 65.0|[24.0124353333333...|\n",
      "|2019-07-28 10:00:00|50.372995936193945| 56.0|[25.4114423333333...|\n",
      "|2019-07-28 11:00:00|46.233539642907274| 49.0|[26.5588219999999...|\n",
      "|2019-07-28 12:00:00| 43.31043530775608| 46.0|[27.6142416666666...|\n",
      "|2019-07-28 13:00:00| 39.68426682886637| 43.0|[28.3597656666666...|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 5.73981\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "rf_predictions = model.transform(testData)\n",
    "\n",
    "# Display example rows\n",
    "rf_predictions.select(\"time\",\"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Compute RMS error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GBR\n",
    "gbr = GBTRegressor(featuresCol=\"features\", labelCol = \"label\", maxIter=50)\n",
    "\n",
    "# Train model using same data structure as RandomForest\n",
    "model = gbr.fit(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+--------------------+\n",
      "|               time|        prediction|label|            features|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "|2019-07-28 04:00:00| 77.35031147143562| 81.0|[19.7834315,9.896...|\n",
      "|2019-07-28 05:00:00| 77.31141289418515| 76.0|[19.8288008333333...|\n",
      "|2019-07-28 06:00:00| 73.86210317246814| 75.0|[20.1969648333333...|\n",
      "|2019-07-28 07:00:00| 68.44148026640768| 78.0|[21.2634120000000...|\n",
      "|2019-07-28 08:00:00|  61.5352699921056| 75.0|[22.6841473333333...|\n",
      "|2019-07-28 09:00:00|  60.6057524956332| 65.0|[24.0124353333333...|\n",
      "|2019-07-28 10:00:00| 53.84249972178284| 56.0|[25.4114423333333...|\n",
      "|2019-07-28 11:00:00| 50.56619925594794| 49.0|[26.5588219999999...|\n",
      "|2019-07-28 12:00:00|  46.8183786561112| 46.0|[27.6142416666666...|\n",
      "|2019-07-28 13:00:00|40.686906150113124| 43.0|[28.3597656666666...|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 3.99015\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "gbr_predictions = model.transform(testData)\n",
    "\n",
    "# Display example rows\n",
    "gbr_predictions.select(\"time\",\"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(gbr_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results from RF, GBR\n",
    "rf_predictions = rf_predictions.drop(\"features\").withColumnRenamed(\"prediction\", \"rf_prediction_3\")\n",
    "gbr_predictions = gbr_predictions.drop(\"label\",\"features\").withColumnRenamed(\"prediction\", \"gbr_prediction_3\")\n",
    "\n",
    "concatenated = rf_predictions.join(gbr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress data points into feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"rf_prediction_3\", \"gbr_prediction_3\"],outputCol=\"features\")\n",
    "ndf = assembler.transform(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create df with temperature_Y and feature vector\n",
    "ndf = newdf.selectExpr(\"label\", \"features\")\n",
    "ndf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n",
    "newdf = newdf.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"time\")))\n",
    "\n",
    "trainingData = newdf.where(\"rank <= .7\").drop(\"rank\")\n",
    "testData = newdf.where(\"rank > .7\").drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\")\n",
    "model = rf.fit(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+--------------------+\n",
      "|               time|        prediction|label|            features|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "|2019-07-28 04:00:00| 76.62317284300812| 81.0|[19.7834315,9.896...|\n",
      "|2019-07-28 05:00:00| 76.62317284300812| 76.0|[19.8288008333333...|\n",
      "|2019-07-28 06:00:00| 73.79351481959176| 75.0|[20.1969648333333...|\n",
      "|2019-07-28 07:00:00| 67.34627463755837| 78.0|[21.2634120000000...|\n",
      "|2019-07-28 08:00:00| 60.65881085478945| 75.0|[22.6841473333333...|\n",
      "|2019-07-28 09:00:00| 54.91606831778425| 65.0|[24.0124353333333...|\n",
      "|2019-07-28 10:00:00|50.372995936193945| 56.0|[25.4114423333333...|\n",
      "|2019-07-28 11:00:00|46.233539642907274| 49.0|[26.5588219999999...|\n",
      "|2019-07-28 12:00:00| 43.31043530775608| 46.0|[27.6142416666666...|\n",
      "|2019-07-28 13:00:00| 39.68426682886637| 43.0|[28.3597656666666...|\n",
      "+-------------------+------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 5.73981\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "rf_predictions = model.transform(testData)\n",
    "\n",
    "# Display example rows\n",
    "rf_predictions.select(\"time\", \"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Compute RMS error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GBR\n",
    "gbr = GBTRegressor(featuresCol=\"features\", labelCol = \"label\", maxIter=50)\n",
    "\n",
    "# Train model using same data structure as RandomForest\n",
    "model = gbr.fit(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----+--------------------+\n",
      "|               time|       prediction|label|            features|\n",
      "+-------------------+-----------------+-----+--------------------+\n",
      "|2019-07-28 04:00:00|77.35031147143562| 81.0|[19.7834315,9.896...|\n",
      "|2019-07-28 05:00:00|77.31141289418515| 76.0|[19.8288008333333...|\n",
      "|2019-07-28 06:00:00|73.86210317246814| 75.0|[20.1969648333333...|\n",
      "|2019-07-28 07:00:00|68.44148026640768| 78.0|[21.2634120000000...|\n",
      "|2019-07-28 08:00:00| 61.5352699921056| 75.0|[22.6841473333333...|\n",
      "+-------------------+-----------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 3.99015\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "gbr_predictions = model.transform(testData)\n",
    "\n",
    "# Display example rows\n",
    "gbr_predictions.select(\"time\",\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(gbr_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
