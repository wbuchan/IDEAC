{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports libraries and loads data into pandas dataframes (Same as main program)\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas.io.json import json_normalize\n",
    "from pandas.tseries import converter as pdtc\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.units as munits\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout, Lambda, BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import keras\n",
    "import json\n",
    "import os\n",
    "import sklearn\n",
    "import numpy\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#Initialize spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ensemble_learning_sparktest\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def concatenate_json_to_df(directory_path, to_output, folder_output):\n",
    "    list_json = []\n",
    "    \n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.split(\".\")[-1] == \"json\":\n",
    "            \n",
    "            with open(directory_path+file, 'r') as f:\n",
    "            \n",
    "                # If json file is not empty\n",
    "                if os.stat(directory_path+file).st_size != 0:\n",
    "                                \n",
    "                        \n",
    "                    # For Observation file type\n",
    "                    if directory_path.split(\"/\")[-2] == \"Observations\":\n",
    "                        dictio = json.load(f)\n",
    "                                            \n",
    "                        if 'observation' in dictio:\n",
    "                            observation = dictio[\"observation\"]\n",
    "                        \n",
    "                            for entry in observation:\n",
    "                                entry['date'] =  file.rstrip(\".json\")\n",
    "                                entry['station'] = dictio['station']\n",
    "                                entry['time'] = datetime.utcfromtimestamp(entry['time']).strftime('%d-%m-%Y %H:%M:%S')\n",
    "                                list_json.append(entry)\n",
    "                    \n",
    "                    \n",
    "                    # For Analyses file type\n",
    "                    elif directory_path.split(\"/\")[-2] == \"Analyses\":\n",
    "                        dictio = json.load(f)\n",
    "                        if 'source' in dictio and 'temperature' in dictio:\n",
    "                            source = list(dictio['source'])\n",
    "                            temperature = list(dictio['temperature'])\n",
    "                            \n",
    "                            dictio['date'] = file.rstrip(\".json\")\n",
    "                            dictio.pop('source', None)\n",
    "                            dictio.pop('temperature', None)\n",
    "                            \n",
    "                            entry = dict()\n",
    "                            for i in range(0, len(source)):\n",
    "                                dictio[source[i]] = temperature[i]\n",
    "                                list_json.append(dictio)\n",
    "\n",
    "                                \n",
    "                    # For Previsions file type\n",
    "                    elif \"Previsions\" in directory_path.split(\"/\"):\n",
    "                        dictio = json.load(f)\n",
    "                        \n",
    "                        latit_r = None\n",
    "                        long_r = None\n",
    "                        \n",
    "                        if \"longitude_r\" in dictio:\n",
    "                            long_r = dictio['longitude_r']\n",
    "\n",
    "                        if \"latitude_r\" in dictio:\n",
    "                            latit_r = dictio['latitude_r']\n",
    "                            \n",
    "                        if 'forecast' in dictio:\n",
    "                            forecast = dictio['forecast'] \n",
    "                            \n",
    "                            \n",
    "                            for i in range(0, len(forecast)):\n",
    "                                dictio = dict()\n",
    "                                \n",
    "                                if long_r != None and latit_r != None:\n",
    "                                    dictio['latitude_r'] = latit_r\n",
    "                                    dictio['longitude_r'] = long_r\n",
    "                                \n",
    "                                if \"temperature\" in forecast[i]:\n",
    "                                    dictio[\"temperature\"] = forecast[i][\"temperature\"]\n",
    "                                    \n",
    "                                if \"time\" in forecast[i]:\n",
    "                                    dictio[\"time\"] = datetime.utcfromtimestamp(forecast[i][\"time\"]).strftime('%d-%m-%Y %H:%M:%S')\n",
    "                                    \n",
    "                                if \"wind\" in forecast[i]:\n",
    "                                    dictio[\"wind\"] = forecast[i][\"wind\"]\n",
    "                                \n",
    "                                if \"wind_dir\" in forecast[i]:\n",
    "                                    dictio[\"wind_dir\"] = forecast[i][\"wind_dir\"]\n",
    "                                \n",
    "                                if \"humidity\" in forecast[i]:\n",
    "                                    dictio[\"humidity\"] = forecast[i][\"humidity\"]\n",
    "                                \n",
    "                                if \"pressure\" in forecast[i]:\n",
    "                                            dictio[\"pressure\"] = forecast[i][\"pressure\"]\n",
    "                                        \n",
    "                                if \"cloud_cover\" in forecast[i]:\n",
    "                                            dictio[\"cloud_cover\"] = forecast[i][\"cloud_cover\"]\n",
    "\n",
    "    \n",
    "                                \n",
    "                                dictio[\"date\"] = file.rstrip(\".json\")\n",
    "                                \n",
    "                                list_json.append(dictio)\n",
    "                \n",
    "                \n",
    "                else:\n",
    "                    dictio = dict()\n",
    "                    dictio[\"date\"] = file.rstrip(\".json\")\n",
    "                    list_json.append(dictio)\n",
    "                                    \n",
    "    df = pd.DataFrame(list_json)\n",
    "    df.drop_duplicates()\n",
    "    df.to_csv(folder_output+\"/\"+to_output.replace(\"/\", \"-\")+\".csv\", sep=\";\", index=False)\n",
    "    \n",
    "    \n",
    "dirname = \"technical_challenge_v9\"\n",
    "if not os.path.exists(dirname):\n",
    "    os.mkdir(dirname)\n",
    "    \n",
    "concatenate_json_to_df('technical_challenge/Asport CAPRED/Previsions/Api-Agro/', \"Asport CAPRED/Previsions/Api-Agro\", dirname)\n",
    "# Launch data of Api, Dark and weatherbit forcesaters for the station Asport\n",
    "path = \"technical_challenge_v9/Asport CAPRED-Previsions-\"\n",
    "path_previsions = [\"Api-Agro.csv\", \"Dark Sky.csv\", \"Weatherbit.csv\"]\n",
    "path_observations = \"technical_challenge_v9/Asport CAPRED-Observations.csv\"\n",
    "\n",
    "def get_forecast(path, path_previsions, path_observations):\n",
    "    list_data = []\n",
    "    for forecaster in path_previsions:\n",
    "        dataframe = pd.read_csv(path + forecaster, sep=\";\")\n",
    "        dataframe[\"time\"] = pd.to_datetime(dataframe['time'], format='%d-%m-%Y %H:%M:%S')\n",
    "        dataframe.sort_values(by=['time'])\n",
    "        dataframe['time'] = dataframe['time'].apply(lambda x: x.replace(minute=0, second=0))\n",
    "        dataframe = dataframe.groupby(['time']).apply(lambda x : x.iloc[:6].mean())\n",
    "        dataframe.columns = [x + \"_\" + forecaster.split('.')[0] for x in dataframe.columns]\n",
    "        list_data.append(dataframe)   \n",
    "    \n",
    "    observation_data = pd.read_csv(path_observations, sep=\";\", index_col=False).drop(columns=[\"station\"]).dropna()\n",
    "    observation_data[\"time\"] = pd.to_datetime(observation_data['time'], format='%d-%m-%Y %H:%M:%S')\n",
    "    observation_data.sort_values(by='time')\n",
    "    observation_data['time'] = observation_data['time'].apply(lambda x: x.replace(minute=0, second=0))\n",
    "    observation_data = observation_data.groupby(['time']).apply(lambda x : x.iloc[0])\n",
    "    observation_data=observation_data.rename(index=str, columns={\"humidity\": \"humidity_Y\", \"precipitation\": \"precipitation_Y\", \"temperature\":\"temperature_Y\"})\n",
    "    \n",
    "    return list_data, observation_data\n",
    "\n",
    "# Associate X to previsions and Y to observations \n",
    "list_data, observation_data = get_forecast(path, path_previsions, path_observations)\n",
    "X = pd.concat(list_data, axis=1)\n",
    "Y = observation_data\n",
    "\n",
    "# Delete non useful variable in prevision data\n",
    "X = X.loc[:, ~X.columns.str.startswith('l')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('c')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('p')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('wind_dir')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('prec')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('date')]\n",
    "X = X.loc[:, ~X.columns.str.startswith('time')]\n",
    "\n",
    "# Delete non useful variable in observation data\n",
    "Y = Y.drop(['date','time'], axis=1)\n",
    "\n",
    "# Import JSON API in order to collect most recent data\n",
    "import json,urllib.request\n",
    "data_A = urllib.request.urlopen(\"http://sd-59247.dedibox.fr/agriscope/obs/transform_obs.php?STATION=Asport\").read()\n",
    "output_A = json.loads(data_A)\n",
    "\n",
    "# Transform JSON data in a dataframe A\n",
    "A = json.loads(data_A)\n",
    "A_data = pd.DataFrame(A)\n",
    "A = A_data.T\n",
    "\n",
    "# Delelte the index of the dataframe A\n",
    "A.index.name = 'time'\n",
    "A.reset_index(level=0, inplace=True)\n",
    "\n",
    "# Convert date time in correct format \n",
    "A['time'] = pd.to_datetime(A['time'])\n",
    "A['time'] = pd.to_datetime(A['time'],'%d-%m-%Y %H:%M:%S')\n",
    "A['time'] = A['time'].apply(lambda x: x.replace(minute=0, second=0))\n",
    "\n",
    "# Rename dataframe A columns\n",
    "A.columns = ['time', 'humidity_Y', 'precipitation_Y', 'temperature_Y']\n",
    "\n",
    "# Reset the index of the dataframe A\n",
    "A = A.set_index('time') \n",
    "\n",
    "# Concat vertically Y and A (Y contains former observation and A most recent one)\n",
    "frames = [Y, A]\n",
    "first_concat = pd.concat(frames)\n",
    "\n",
    "# Merge X and Y dataframe in one single dataframe \"df_forcast_cap\"\n",
    "df_forcast_cap = pd.merge(X, first_concat, right_index=True, left_index=True)\n",
    "df_forcast_cap = df_forcast_cap.drop(['precipitation_Y'], axis=1)\n",
    "\n",
    "# Delete missing values for \"df_forcast_cap\"\n",
    "df_forcast_cap = df_forcast_cap.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyarrow optimization for pandas/spark conversion \n",
    "\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") NEED TO FIX PyARROW VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/will/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/session.py:753: UserWarning:\n",
      "\n",
      "createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 0.15.1 must be installed; however, your version was 0.13.0.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[temperature_Api-Agro: double, wind_Api-Agro: double, humidity_Api-Agro: double, humidity_Dark Sky: double, temperature_Dark Sky: double, wind_Dark Sky: double, humidity_Weatherbit: double, temperature_Weatherbit: double, wind_Weatherbit: double, humidity_Y: double, temperature_Y: double]\n"
     ]
    }
   ],
   "source": [
    "#convert pandas to dataframe\n",
    "train_df = spark.createDataFrame(df_forcast_cap) #dataframe\n",
    "\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temperature_Api-Agro: double (nullable = true)\n",
      " |-- wind_Api-Agro: double (nullable = true)\n",
      " |-- humidity_Api-Agro: double (nullable = true)\n",
      " |-- humidity_Dark Sky: double (nullable = true)\n",
      " |-- temperature_Dark Sky: double (nullable = true)\n",
      " |-- wind_Dark Sky: double (nullable = true)\n",
      " |-- humidity_Weatherbit: double (nullable = true)\n",
      " |-- temperature_Weatherbit: double (nullable = true)\n",
      " |-- wind_Weatherbit: double (nullable = true)\n",
      " |-- humidity_Y: double (nullable = true)\n",
      " |-- temperature_Y: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compress data points into feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"temperature_Api-Agro\", \"wind_Api-Agro\", \"humidity_Api-Agro\", \\\n",
    "                                      \"humidity_Dark Sky\", \"temperature_Dark Sky\", \"wind_Dark Sky\", \\\n",
    "                                      \"humidity_Weatherbit\", \"temperature_Weatherbit\", \"wind_Weatherbit\", \\\n",
    "                                      \"humidity_Y\", \"temperature_Y\"],outputCol=\"features\")\n",
    "newdf = assembler.transform(train_df)\n",
    "\n",
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creates df with temperature_Y and feature vector\n",
    "newdf = newdf.selectExpr(\"temperature_Y as label\", \"features as features\")\n",
    "\n",
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "(trainingData, testData) = newdf.randomSplit([0.7, 0.3])   \n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"features\")\n",
    "model = rf.fit(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "| 33.70148060517556| 30.0|[13.6412908333333...|\n",
      "|35.672619179325146| 32.0|[12.6293786666666...|\n",
      "| 34.26207057076552| 33.0|[14.70763075,16.9...|\n",
      "|  36.7622389664255| 37.0|[14.3207596666666...|\n",
      "| 39.43285112812153| 38.0|[18.2214705,11.00...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 2.28889\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Compute RMS error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1733.fit.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2108)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:126)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.$anonfun$train$2(DecisionTreeRegressor.scala:143)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:137)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:310)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:51)\n\tat org.apache.spark.ml.regression.GBTRegressor.$anonfun$train$1(GBTRegressor.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:168)\n\tat org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:152)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-b139bdafd782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train model using same data structure as RandomForest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1733.fit.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2108)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:126)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.$anonfun$train$2(DecisionTreeRegressor.scala:143)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:137)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:310)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:51)\n\tat org.apache.spark.ml.regression.GBTRegressor.$anonfun$train$1(GBTRegressor.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:168)\n\tat org.apache.spark.ml.regression.GBTRegressor.train(GBTRegressor.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:152)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# GBR \n",
    "\n",
    "# Initialize GBT\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol = \"label\", maxIter=10)\n",
    "\n",
    "# Train model using same data structure as RandomForest\n",
    "model = gbt.fit(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
